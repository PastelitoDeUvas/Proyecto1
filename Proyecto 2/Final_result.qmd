---
title: "Actividad: Regresi√≥n Lineal con T√©rminos Cuadr√°ticos"
format:
  html:
    theme: minty  # Puedes probar tambi√©n "journal" o "flatly"
    css: estilo.css

editor: visual
---

## Contexto

Eres parte de un equipo de cient√≠ficos de datos en una startup ambiental que busca predecir el √≠ndice de calidad del aire (AQI) en ciudades de Europa. Los datos simulados contienen 5 variables predictoras:

-   **NO2** (di√≥xido de nitr√≥geno)
-   **PM10** (part√≠culas en suspensi√≥n)
-   **SO2** (di√≥xido de azufre)
-   **CO** (mon√≥xido de carbono)
-   **O3** (ozono)

Tu prop√≥sito es modelar el AQI a partir de estas variables para predecir la calidad del aire en tiempo real. Para esto, debes implementar un modelo de regresi√≥n lineal que mejor se ajuste a los datos y evaluar su desempe√±o con validaci√≥n cruzada. El modelo puede incluir t√©rminos cuadr√°ticos, c√∫bicos o combinaciones de estas transformaciones para mejorar su precisi√≥n.

------------------------------------------------------------------------

## Preguntas

### 1. An√°lisis Exploratorio y Preprocesamiento

-   **a)** Realice un an√°lisis exploratorio de los datos. En este caso, todas las variables son importantes para el modelo. Para este an√°lisis, se recomienda visualizar la **correlaci√≥n** de las variables entre ellas y con el AQI. ---

```{r message=FALSE,warning=FALSE,echo=FALSE,include=FALSE}
# Cargar librer√≠as necesarias

ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}
packages <- c("readxl","tidyverse","GGally","dplyr","reticulate","knitr","kableExtra")
ipak(packages)


```

```{r }
#| code-fold: true
data <- read.csv('data.csv')

#General inspection (unicamente un formalismo)
kable(data[1:5,]) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed"))

normalize_zscore <- function(x) {
  return((x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE))
}



test <- data %>% sample_n(1000)

test_standardized <- test %>%
  mutate(across(c(NO2, PM10, SO2, CO, O3, AQI), normalize_zscore))

#distribution analysis 
test_standardized %>%
  pivot_longer(cols = c(NO2, PM10, SO2, CO, O3, AQI), names_to = "Variable", values_to = "Valor") %>%
  ggplot(aes(x = Valor, fill = Variable)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~Variable, scales = "free") +
  theme_minimal()

#Variable Relation 

ggpairs(test_standardized, columns = c("NO2", "PM10", "SO2", "CO", "O3", "AQI"), aes(alpha = 0.5))
corr_matrix <- cor(test_standardized %>% select(NO2, PM10, SO2, CO, O3, AQI)) 
corrplot::corrplot(corr_matrix, method = "color", type = "lower", tl.col = "black", addCoef.col = "black")





```

### 2. Ecuaci√≥n Normal

**a) Deriva matem√°ticamente la ecuaci√≥n normal para \\( \\theta \\), incluyendo t√©rminos cuadr√°ticos, c√∫bicos, etc.:**

$$
\hat{\beta} = (X^\top X)^{-1} X^\top y
$$ El error cuadratico esta dado por :

$$
\varepsilon(\beta) = || y - X\beta ||^2
$$

Naturalmente como y y x no estan en el mismo plano , lo que necesitamos es sumarle un error \varepsilon , para lograr que este error sea el menor posible es necesario que sea ortogonal al plano , y eso solo se consigue en los puntos donde su derivada es cero, por tanto derivamos \varepsilon con respecto a \beta

$$
\frac{d\varepsilon}{d\beta} = -2 X^\top (y - X\beta)
$$

Ya que tenemos la derivada , encontramos el punto critico igualando a cero

$$
X^\top (y - X\beta) = 0
$$

Resolviendo para ( \beta ):

$$
X^\top X \beta = X^\top y
$$

Lo que en efecto nos da la ecuacion normal $$
\beta = (X^\top X)^{-1} X^\top y
$$ **b) Si X\^**\top X , ¬øc√≥mo es la condici√≥n de la matriz? ¬øTiene alg√∫n problema este n√∫mero de condici√≥n?

La condici√≥n de la matriz ( X\^\top X ) se eval√∫a mediante el **n√∫mero de condici√≥n**, que es una m√©trica que indica cu√°n sensible es la soluci√≥n de un sistema de ecuaciones a peque√±as perturbaciones en los datos.

**Condici√≥n de la matriz ( X\^**\top X ) El n√∫mero de condici√≥n de una matriz ( A ) se define como:

$$
\kappa(A) = \frac{\sigma_{\max}}{\sigma_{\min}}
$$

donde ( \sigma*{*\max} ) y ( \sigma{\min} ) son los valores singulares m√°ximo y m√≠nimo de ( A ), respectivamente.

-   Si ( \kappa(X\^\top X) ) es **bajo**, la matriz est√° bien condicionada y el c√°lculo de ( (X\top X){-1} ) es estable.
-   Si ( \kappa(X\^\top X) ) es **alto**, la matriz est√° mal condicionada, lo que significa que peque√±as variaciones en los datos pueden causar grandes cambios en ( \beta ), afectando la precisi√≥n del modelo. **¬øTiene alg√∫n problema este n√∫mero de condici√≥n?**

Calculamos el numero de condicion mediante numpy :

```{python}
#| code-fold: true
import sys

sys.path.append("C:\\Users\\JUAN\\Desktop\\Algebra-lineal\\Proyecto 1 (K-means)\\repositorio\\Proyecto 2")
import codes as cd
import pandas as pd

data = pd.read_csv("data.csv")
data = data.values 


data = cd.standardize(data)
a=cd.quitar_ultima_columna(data)
b=cd.result_vector(data)
cd.condicion(a)


```

Dado que el numero de condicion es cercano a 1 ,podemos afirmar que la matriz esta bien condicionada , al tener sus valores singulares maximos y minmos muy cerca entre si , por lo que peque√±as perturbaciones no afectan la matriz

**c) ¬øQu√© ventajas tiene la ecuaci√≥n normal sobre el gradiente descendente?**

Las principales ventajas que tiene la ecuaci√≥n normal sobre el gradiente son:

-   Da la soluci√≥n √≥ptima directamente, a diferencia del gradiente que requiere de un proceso iterativo.\
-   No requiere de hiperpar√°metros, mientras que el gradiente necesita elegir una tasa de aprendizaje adecuada.\
-   La implementaci√≥n en c√≥digo es m√°s sencilla que la del gradiente.\
-   Dado que solo usamos 5 caracter√≠sticas, la complejidad de la ecuaci√≥n normal es ( 5\^3 = 125 ), mientras que la del gradiente es ( 5 \times 10000 = 50000 ).\
    En este caso, es mucho m√°s eficiente aplicar la ecuaci√≥n normal que el gradiente descendente.

**d) Implementaci√≥n del modelo**

Implemente un algoritmo que:

1.  Divida los datos en 80% para entrenamiento y 20% para prueba.\
2.  Use √∫nicamente la librer√≠a `numpy`.\
3.  Calcule los coeficientes ( \hat{\beta} ) de la regresi√≥n lineal, incluyendo t√©rminos cuadr√°ticos, c√∫bicos, etc., para mejorar el ajuste del modelo.\
4.  Seleccione el modelo que mejor se ajuste a los datos de prueba.

```{python }
#| code-fold: true
import numpy as np
err,beta,grado=cd.pseudo_training(a,b)
error_r2=cd.r2(a,b,beta)


print("=" * 40)
print("         Resultados del Modelo         ")
print("=" * 40)
print(f"Error cuadr√°tico  : {err:.4f}")
print(f"Grado del modelo  : {grado}")
print("Coeficientes beta :")

# Asegurar que beta es una lista para evitar errores de formato
beta = np.array(beta).flatten().tolist()  

for i, b in enumerate(beta, start=1):
    print(f"   Œ≤{i}: {b:.4f}")

print(f"Coeficiente R¬≤    : {error_r2:.4f}")
print("=" * 40)


```

### 3. Gradiente Descendente

**a) Explica por qu√© se normalizan las caracter√≠sticas antes de aplicar gradiente descendente** Es importante normalizar las caracteristicas antes de aplicar el modeelo debido a las siguientes razones: -Evita que alguna de las caracteristicas domine el entrenamiento del modelo,debido a que si una de las caracteristicas esta en millones y otra entre 1y 0 , la que esta en millones tendra mucho mas peso que la otra , por lo que su convergencia seria mas lenta y su error mayor -acelera la convergencia ya que con cada iteracion ajusta el peso de cada caracteristica, por lo que si sus escalas son muy diferentes , esto puede resultar en un error mayor y una convergencia mas lenta -Los calculos del gradiente pueden verse afectados por valores extremos

**b) Implementa un algoritmo de gradiente descendente para regresi√≥n lineal, unicamente usando ‚Äúnumpy‚Äù, incluyendo t√©rminos cuadr√°ticos, c√∫bicos, etc. para mejorar el ajuste del modelo. para mejorar el ajuste del modelo. Escoja el modelo que mejor se ajuste a los datos test.**

```{python}
#| code-fold: true
import numpy as np
data = cd.standardize(data)
a=cd.quitar_ultima_columna(data)
b=cd.result_vector(data)
b = b.reshape(-1, 1)



err,beta,grado,iteration=cd.gradiente_training(a,b)
error_r2=cd.r2(a,b,beta)

print("=" * 50)
print("             Resultados del Modelo             ")
print("=" * 50)
print(f"Error cuadr√°tico   : {err:.4f}")
print(f"Grado del modelo   : {grado}")
print(f"Iteraciones        : {iteration}")
print("Coeficientes beta  :")

# Asegurar que beta es una lista para evitar errores de formato

beta = np.array(beta).flatten().tolist()  

for i, b in enumerate(beta, start=1):
    print(f"   Œ≤{i}: {b:.4f}")

print(f"Coeficiente R¬≤     : {error_r2:.4f}")
print("=" * 50)

```

**c) si los modelos de gradiente descendente y ecuaci√≥n normal son diferentes, ¬øcu√°l es el motivo?, y si son iguales, ¬øcual tiene mejor desempe√±o?**

Ambos modelos dan resultados casi identicos con una diferencia en su error de menos de 10\^-8 , pero dada la complejidad computacional de cada uno , lo mejor es tomar el de ecuaaciones normales ,que converge de forma mas rapida y con menos trabajo

**c) ¬øQu√© ventajas tiene el gradiente descendente sobre la ecuaci√≥n normal?**

-Cuando se trata de grandes numeros de datos con muchas caracteristicas , el gradiente descendiente tiene una complejidad mucho menor a las ecuaciones normales , por lo que requiere de menoos trabajo computacional

-Si se trata de un grupo de datos muy grande con un gran numero de caracteristicas ,el gradiente descendiente lograra converger a un resultado de forma mas rapida que las ecuaciones normales

-Requiere de un uso menor de memoria al solo almacenar los pesos y los parametros, a diferencia de las ecuaciones normales que necesitan manipular y almacenar todas las matrices por completo

**d) Si el MSE no disminuye despu√©s de 200 √©pocas, ¬øqu√© hiperpar√°metros ajustar√≠as?**

Si el MSE no disminuye despues de 200 epocas es porque debe estar atascado debido a que la tasa de aprendizaje es muy baja , lo que genera que los cambios en los pesos sean muy peque√±os o casi insignificantes , resultando en periodos muy largos para converger , o incluso se puede dar el caso de que no logre converger

### 4. Cross Validation

**b) ¬øQu√© ventajas tiene la validaci√≥n cruzada sobre la divisi√≥n de los datos en entrenamiento y test?**

-Aprovechamiento total de los datos: Todos los datos se utilizan tanto para entrenar como para validar, lo que permite obtener una evaluaci√≥n m√°s representativa del modelo.

-Reducci√≥n del sesgo en la evaluaci√≥n: Minimiza el impacto de una mala divisi√≥n de los datos, ya que el modelo se prueba con diferentes subconjuntos del dataset.

-Mayor utilidad en datasets peque√±os: Es especialmente beneficiosa cuando hay pocos datos, ya que evita desperdiciar informaci√≥n al no reservar un √∫nico conjunto de test fijo.

Este enfoque garantiza una evaluaci√≥n m√°s robusta del desempe√±o del modelo y reduce la variabilidad en los resultados.

### 5. Evaluaci√≥n del Modelo

**a) Eval√∫e el desempe√±o de los modelos implementados en los puntos 2 y 3, usando el error cuadr√°tico medio (MSE) y el coeficiente de determinaci√≥n (ùëÖ2). ¬øQu√© modelo tienemejor desempe√±o?**

Al comparar los resultados obtenidos con las ecuaciones normales y el descenso de gradiente, se observa que las ecuaciones normales ofrecen un modelo con un error cuadr√°tico medio (MSE) menor, lo que indica un mejor ajuste en t√©rminos de minimizaci√≥n del error absoluto.

Adem√°s, el MSE mide el error en las mismas unidades que la variable objetivo, permitiendo interpretar directamente la magnitud de las desviaciones del modelo. En contraste, ùëÖ\^2 es una m√©trica relativa, lo que significa que solo muestra qu√© porcentaje de la variabilidad de los datos es explicado por el modelo, sin proporcionar informaci√≥n sobre el error absoluto real.

En este caso, el MSE ha resultado ser una mejor m√©trica para evaluar el desempe√±o del modelo, ya que refleja con mayor precisi√≥n la calidad del ajuste en t√©rminos de los errores absolutos cometidos por la regresi√≥n.
