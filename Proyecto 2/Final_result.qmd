---
title: "Actividad: Regresión Lineal con Términos Cuadráticos"
format:
  html:
    theme: minty  # Puedes probar también "journal" o "flatly"
    css: estilo.css

editor: visual
---

## Contexto

Eres parte de un equipo de científicos de datos en una startup ambiental que busca predecir el índice de calidad del aire (AQI) en ciudades de Europa. Los datos simulados contienen 5 variables predictoras:

-   **NO2** (dióxido de nitrógeno)
-   **PM10** (partículas en suspensión)
-   **SO2** (dióxido de azufre)
-   **CO** (monóxido de carbono)
-   **O3** (ozono)

Tu propósito es modelar el AQI a partir de estas variables para predecir la calidad del aire en tiempo real. Para esto, debes implementar un modelo de regresión lineal que mejor se ajuste a los datos y evaluar su desempeño con validación cruzada. El modelo puede incluir términos cuadráticos, cúbicos o combinaciones de estas transformaciones para mejorar su precisión.

------------------------------------------------------------------------

## Preguntas

### 1. Análisis Exploratorio y Preprocesamiento

-   **a)** Realice un análisis exploratorio de los datos. En este caso, todas las variables son importantes para el modelo. Para este análisis, se recomienda visualizar la **correlación** de las variables entre ellas y con el AQI. ---

```{r message=FALSE,warning=FALSE,echo=FALSE,include=FALSE}
# Cargar librerías necesarias

ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}
packages <- c("readxl","tidyverse","GGally","dplyr","reticulate","knitr","kableExtra")
ipak(packages)


```

```{r }
#| code-fold: true
data <- read.csv('data.csv')

#General inspection (unicamente un formalismo)
kable(data[1:5,]) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed"))

normalize_zscore <- function(x) {
  return((x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE))
}



test <- data %>% sample_n(1000)

test_standardized <- test %>%
  mutate(across(c(NO2, PM10, SO2, CO, O3, AQI), normalize_zscore))

#distribution analysis 
test_standardized %>%
  pivot_longer(cols = c(NO2, PM10, SO2, CO, O3, AQI), names_to = "Variable", values_to = "Valor") %>%
  ggplot(aes(x = Valor, fill = Variable)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~Variable, scales = "free") +
  theme_minimal()

#Variable Relation 

ggpairs(test_standardized, columns = c("NO2", "PM10", "SO2", "CO", "O3", "AQI"), aes(alpha = 0.5))
corr_matrix <- cor(test_standardized %>% select(NO2, PM10, SO2, CO, O3, AQI)) 
corrplot::corrplot(corr_matrix, method = "color", type = "lower", tl.col = "black", addCoef.col = "black")





```

### 2. Ecuación Normal

**a) Deriva matemáticamente la ecuación normal para \\( \\theta \\), incluyendo términos cuadráticos, cúbicos, etc.:**

$$
\hat{\beta} = (X^\top X)^{-1} X^\top y
$$ El error cuadratico esta dado por :

$$
\varepsilon(\beta) = || y - X\beta ||^2
$$

Naturalmente como y y x no estan en el mismo plano , lo que necesitamos es sumarle un error \varepsilon , para lograr que este error sea el menor posible es necesario que sea ortogonal al plano , y eso solo se consigue en los puntos donde su derivada es cero, por tanto derivamos \varepsilon con respecto a \beta

$$
\frac{d\varepsilon}{d\beta} = -2 X^\top (y - X\beta)
$$

Ya que tenemos la derivada , encontramos el punto critico igualando a cero

$$
X^\top (y - X\beta) = 0
$$

Resolviendo para ( \beta ):

$$
X^\top X \beta = X^\top y
$$

Lo que en efecto nos da la ecuacion normal $$
\beta = (X^\top X)^{-1} X^\top y
$$ **b) Si X\^**\top X , ¿cómo es la condición de la matriz? ¿Tiene algún problema este número de condición?

La condición de la matriz ( X\^\top X ) se evalúa mediante el **número de condición**, que es una métrica que indica cuán sensible es la solución de un sistema de ecuaciones a pequeñas perturbaciones en los datos.

**Condición de la matriz ( X\^**\top X ) El número de condición de una matriz ( A ) se define como:

$$
\kappa(A) = \frac{\sigma_{\max}}{\sigma_{\min}}
$$

donde ( \sigma*{*\max} ) y ( \sigma{\min} ) son los valores singulares máximo y mínimo de ( A ), respectivamente.

-   Si ( \kappa(X\^\top X) ) es **bajo**, la matriz está bien condicionada y el cálculo de ( (X\top X){-1} ) es estable.
-   Si ( \kappa(X\^\top X) ) es **alto**, la matriz está mal condicionada, lo que significa que pequeñas variaciones en los datos pueden causar grandes cambios en ( \beta ), afectando la precisión del modelo. **¿Tiene algún problema este número de condición?** Si (X) contiene características altamente correlacionadas, (X\^\top X) puede volverse **casi singular**, resultando en un número de condición alto. Esto puede causar problemas numéricos al calcular ( (X\top X){-1} ), afectando la estabilidad de la ecuación normal.

Para evaluar esto en la práctica, podemos calcular ( \kappa(X\^\top X) ) con `numpy`:

```{python}
#| code-fold: true
import sys

sys.path.append("C:\\Users\\JUAN\\Desktop\\Algebra-lineal\\Proyecto 1 (K-means)\\repositorio\\Proyecto 2")
import codes as cd
import pandas as pd

data = pd.read_csv("data.csv")
data = data.values 


data = cd.standardize(data)
a=cd.quitar_ultima_columna(data)
b=cd.result_vector(data)
cd.condicion(a)

print (data)

```

Dado que el numero de condicion es cercano a 1 ,podemos afirmar que la matriz esta bien condicionada , al tener sus valores singulares maximos y minmos muy cerca entre si , por lo que pequeñas perturbaciones no afectan la matriz

**c) ¿Qué ventajas tiene la ecuación normal sobre el gradiente descendente?**

Las principales ventajas que tiene la ecuación normal sobre el gradiente son:

-   Da la solución óptima directamente, a diferencia del gradiente que requiere de un proceso iterativo.\
-   No requiere de hiperparámetros, mientras que el gradiente necesita elegir una tasa de aprendizaje adecuada.\
-   La implementación en código es más sencilla que la del gradiente.\
-   Dado que solo usamos 5 características, la complejidad de la ecuación normal es ( 5\^3 = 125 ), mientras que la del gradiente es ( 5 \times 10000 = 50000 ).\
    En este caso, es mucho más eficiente aplicar la ecuación normal que el gradiente descendente.

**d) Implementación del modelo**

Implemente un algoritmo que:

1.  Divida los datos en 80% para entrenamiento y 20% para prueba.\
2.  Use únicamente la librería `numpy`.\
3.  Calcule los coeficientes ( \hat{\beta} ) de la regresión lineal, incluyendo términos cuadráticos, cúbicos, etc., para mejorar el ajuste del modelo.\
4.  Seleccione el modelo que mejor se ajuste a los datos de prueba.

```{python }
#| code-fold: true

err,beta,grado=cd.pseudo_training(a,b)

print("error cuadratico: ", err, "beta",  beta ," grado " , grado)


```

### 3. Gradiente Descendente

**a) Explica por qué se normalizan las características antes de aplicar gradiente descendente**
Es importante normalizar las caracteristicas antes de aplicar el modeelo debido a las siguientes razones:
-Evita que alguna de las caracteristicas domine el entrenamiento del modelo,debido a que si una de las caracteristicas esta en millones y otra entre 1y 0 , la que esta en millones tendra mucho mas peso  que la otra , por lo que su convergencia seria mas lenta y su error mayor 
-acelera la convergencia ya que con cada iteracion ajusta el peso de cada caracteristica, por lo que si sus escalas son muy diferentes , esto puede resultar en un error mayor y una convergencia mas lenta
-Los calculos del gradiente pueden verse afectados por valores extremos

**b) Implementa un algoritmo de gradiente descendente para regresión lineal, unicamente usando “numpy”, incluyendo términos cuadráticos, cúbicos, etc. para mejorar el ajuste del modelo. para mejorar el ajuste del modelo. Escoja el modelo que mejor se ajuste a los datos test.**

```{python}
#| code-fold: true
data = cd.standardize(data)
a=cd.quitar_ultima_columna(data)
b=cd.result_vector(data)
b = b.reshape(-1, 1)



err,beta,grado,iteration=cd.gradiente_training(a,b)
print("error",err,"beta",beta,"grado",grado,"iteracion",iteration)
```
**c) si los modelos de gradiente descendente y ecuación normal son diferentes, ¿cuál es el motivo?, y si son iguales, ¿cual tiene mejor desempeño?**

Ambos modelos dan resultados casi identicos con una diferencia en su error de menos de 10^-8 , pero dada la complejidad computacional de cada uno , lo mejor es tomar el de ecuaaciones normales ,que converge de forma mas rapida y con menos trabajo 


**c) ¿Qué ventajas tiene el gradiente descendente sobre la ecuación normal?**

-Cuando se trata de grandes numeros de datos con muchas caracteristicas , el gradiente descendiente tiene una complejidad mucho menor a las ecuaciones normales , por lo que requiere de menoos trabajo computacional 
-Si se trata de un grupo de datos muy grande con un gran numero de caracteristicas ,el gradiente descendiente lograra converger a un resultado de forma mas rapida que las ecuaciones normales 
-Requiere de un uso menor de memoria al solo almacenar los pesos y los parametros, a diferencia de las ecuaciones normales que necesitan manipular y almacenar todas las matrices por completo

**d) Si el MSE no disminuye después de 200 épocas, ¿qué hiperparámetros ajustarías?**

Si el MSE no disminuye despues de 200 epocas es porque debe estar atascado debido a que la tasa de aprendizaje es muy baja , lo que genera que los cambios en los pesos sean muy pequeños o casi insignificantes , resultando en periodos muy largos para converger , o incluso se puede dar el caso de que no logre converger 

### 4. Cross Validation

**b) ¿Qué ventajas tiene la validación cruzada sobre la división de los datos en entrenamiento y test?**

-Todos los datos se usan tanto para validar como para entrenar,por lo que mejora el desempeño del modelo
-Reduce el impacto de una mala division de los datos al evaluar el modelo con diferentes subconjuntos
-Es util cuando hay pocos datos en el data set 

### 5. Evaluación del Modelo

**a) Evalúe el desempeño de los modelos implementados en los puntos 2 y 3, usando el error cuadrático medio (MSE) y el coeficiente de determinación (𝑅2). ¿Qué modelo tienemejor desempeño?**

Dados los resultados en las ecuaciones normales y el gradiente decendiente , se puede ver que las ecuaciones normales ofrencen un modelo con un error cuadratico menor que el gradiente descendiente ,aunque la diferencia entre ambos es menor a 10^-8 , por lo que no es muy significativo realmente en el desempeño del modelo 