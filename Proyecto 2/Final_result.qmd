---
title: "Actividad: Regresión Lineal con Términos Cuadráticos"
format:
  html:
    theme: minty  # Puedes probar también "journal" o "flatly"
    css: estilo.css

editor: visual
---

## Contexto

Eres parte de un equipo de científicos de datos en una startup ambiental que busca predecir el índice de calidad del aire (AQI) en ciudades de Europa. Los datos simulados contienen 5 variables predictoras:

-   **NO2** (dióxido de nitrógeno)
-   **PM10** (partículas en suspensión)
-   **SO2** (dióxido de azufre)
-   **CO** (monóxido de carbono)
-   **O3** (ozono)

Tu propósito es modelar el AQI a partir de estas variables para predecir la calidad del aire en tiempo real. Para esto, debes implementar un modelo de regresión lineal que mejor se ajuste a los datos y evaluar su desempeño con validación cruzada. El modelo puede incluir términos cuadráticos, cúbicos o combinaciones de estas transformaciones para mejorar su precisión.

------------------------------------------------------------------------

## Preguntas

### 1. Análisis Exploratorio y Preprocesamiento

-   **a)** Realice un análisis exploratorio de los datos. En este caso, todas las variables son importantes para el modelo. Para este análisis, se recomienda visualizar la **correlación** de las variables entre ellas y con el AQI. ---

```{r message=FALSE,warning=FALSE,echo=FALSE,include=FALSE}
# Cargar librerías necesarias

ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}
packages <- c("readxl","tidyverse","GGally","dplyr","reticulate","knitr","kableExtra")
ipak(packages)


```

```{r }
#| code-fold: true
data <- read.csv('data.csv')

#General inspection (unicamente un formalismo)
kable(data[1:5,]) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed"))

normalize_zscore <- function(x) {
  return((x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE))
}



test <- data %>% sample_n(1000)

test_standardized <- test %>%
  mutate(across(c(NO2, PM10, SO2, CO, O3, AQI), normalize_zscore))

#distribution analysis 
test_standardized %>%
  pivot_longer(cols = c(NO2, PM10, SO2, CO, O3, AQI), names_to = "Variable", values_to = "Valor") %>%
  ggplot(aes(x = Valor, fill = Variable)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~Variable, scales = "free") +
  theme_minimal()

#Variable Relation 

ggpairs(test_standardized, columns = c("NO2", "PM10", "SO2", "CO", "O3", "AQI"), aes(alpha = 0.5))
corr_matrix <- cor(test_standardized %>% select(NO2, PM10, SO2, CO, O3, AQI)) 
corrplot::corrplot(corr_matrix, method = "color", type = "lower", tl.col = "black", addCoef.col = "black")





```

### 2. Ecuación Normal

**a) Deriva matemáticamente la ecuación normal para \\( \\theta \\), incluyendo términos cuadráticos, cúbicos, etc.:**

$$
\hat{\beta} = (X^\top X)^{-1} X^\top y
$$ El error cuadratico esta dado por :

$$
\varepsilon(\beta) = || y - X\beta ||^2
$$

Naturalmente como y y x no estan en el mismo plano , lo que necesitamos es sumarle un error \varepsilon , para lograr que este error sea el menor posible es necesario que sea ortogonal al plano , y eso solo se consigue en los puntos donde su derivada es cero, por tanto derivamos \varepsilon con respecto a \beta

$$
\frac{d\varepsilon}{d\beta} = -2 X^\top (y - X\beta)
$$

Ya que tenemos la derivada , encontramos el punto critico igualando a cero

$$
X^\top (y - X\beta) = 0
$$

Resolviendo para ( \beta ):

$$
X^\top X \beta = X^\top y
$$

Lo que en efecto nos da la ecuacion normal $$
\beta = (X^\top X)^{-1} X^\top y
$$ **b) Si X\^**\top X , ¿cómo es la condición de la matriz? ¿Tiene algún problema este número de condición?

La condición de la matriz ( X\^\top X ) se evalúa mediante el **número de condición**, que es una métrica que indica cuán sensible es la solución de un sistema de ecuaciones a pequeñas perturbaciones en los datos.

**Condición de la matriz ( X\^**\top X ) El número de condición de una matriz ( A ) se define como:

$$
\kappa(A) = \frac{\sigma_{\max}}{\sigma_{\min}}
$$

donde ( \sigma*{*\max} ) y ( \sigma{\min} ) son los valores singulares máximo y mínimo de ( A ), respectivamente.

-   Si ( \kappa(X\^\top X) ) es **bajo**, la matriz está bien condicionada y el cálculo de ( (X\top X){-1} ) es estable.
-   Si ( \kappa(X\^\top X) ) es **alto**, la matriz está mal condicionada, lo que significa que pequeñas variaciones en los datos pueden causar grandes cambios en ( \beta ), afectando la precisión del modelo. **¿Tiene algún problema este número de condición?**

Calculamos el numero de condicion mediante numpy :

```{python}
#| code-fold: true
import sys

sys.path.append("C:\\Users\\JUAN\\Desktop\\Algebra-lineal\\Proyecto 1 (K-means)\\repositorio\\Proyecto 2")
import codes as cd
import pandas as pd

data = pd.read_csv("data.csv")
data = data.values 


data = cd.standardize(data)
a=cd.quitar_ultima_columna(data)
b=cd.result_vector(data)
cd.condicion(a)


```

Dado que el numero de condicion es cercano a 1 ,podemos afirmar que la matriz esta bien condicionada , al tener sus valores singulares maximos y minmos muy cerca entre si , por lo que pequeñas perturbaciones no afectan la matriz

**c) ¿Qué ventajas tiene la ecuación normal sobre el gradiente descendente?**

Las principales ventajas que tiene la ecuación normal sobre el gradiente son:

-   Da la solución óptima directamente, a diferencia del gradiente que requiere de un proceso iterativo.\
-   No requiere de hiperparámetros, mientras que el gradiente necesita elegir una tasa de aprendizaje adecuada.\
-   La implementación en código es más sencilla que la del gradiente.\
-   Dado que solo usamos 5 características, la complejidad de la ecuación normal es ( 5\^3 = 125 ), mientras que la del gradiente es ( 5 \times 10000 = 50000 ).\
    En este caso, es mucho más eficiente aplicar la ecuación normal que el gradiente descendente.

**d) Implementación del modelo**

Implemente un algoritmo que:

1.  Divida los datos en 80% para entrenamiento y 20% para prueba.\
2.  Use únicamente la librería `numpy`.\
3.  Calcule los coeficientes ( \hat{\beta} ) de la regresión lineal, incluyendo términos cuadráticos, cúbicos, etc., para mejorar el ajuste del modelo.\
4.  Seleccione el modelo que mejor se ajuste a los datos de prueba.

```{python }
#| code-fold: true
import numpy as np
err,beta,grado=cd.pseudo_training(a,b)
error_r2=cd.r2(a,b,beta)


print("=" * 40)
print("         Resultados del Modelo         ")
print("=" * 40)
print(f"Error cuadrático  : {err:.4f}")
print(f"Grado del modelo  : {grado}")
print("Coeficientes beta :")

# Asegurar que beta es una lista para evitar errores de formato
beta = np.array(beta).flatten().tolist()  

for i, b in enumerate(beta, start=1):
    print(f"   β{i}: {b:.4f}")

print(f"Coeficiente R²    : {error_r2:.4f}")
print("=" * 40)


```

### 3. Gradiente Descendente

**a) Explica por qué se normalizan las características antes de aplicar gradiente descendente** Es importante normalizar las caracteristicas antes de aplicar el modeelo debido a las siguientes razones: -Evita que alguna de las caracteristicas domine el entrenamiento del modelo,debido a que si una de las caracteristicas esta en millones y otra entre 1y 0 , la que esta en millones tendra mucho mas peso que la otra , por lo que su convergencia seria mas lenta y su error mayor -acelera la convergencia ya que con cada iteracion ajusta el peso de cada caracteristica, por lo que si sus escalas son muy diferentes , esto puede resultar en un error mayor y una convergencia mas lenta -Los calculos del gradiente pueden verse afectados por valores extremos

**b) Implementa un algoritmo de gradiente descendente para regresión lineal, unicamente usando “numpy”, incluyendo términos cuadráticos, cúbicos, etc. para mejorar el ajuste del modelo. para mejorar el ajuste del modelo. Escoja el modelo que mejor se ajuste a los datos test.**

```{python}
#| code-fold: true
import numpy as np
data = cd.standardize(data)
a=cd.quitar_ultima_columna(data)
b=cd.result_vector(data)
b = b.reshape(-1, 1)



err,beta,grado,iteration=cd.gradiente_training(a,b)
error_r2=cd.r2(a,b,beta)

print("=" * 50)
print("             Resultados del Modelo             ")
print("=" * 50)
print(f"Error cuadrático   : {err:.4f}")
print(f"Grado del modelo   : {grado}")
print(f"Iteraciones        : {iteration}")
print("Coeficientes beta  :")

# Asegurar que beta es una lista para evitar errores de formato

beta = np.array(beta).flatten().tolist()  

for i, b in enumerate(beta, start=1):
    print(f"   β{i}: {b:.4f}")

print(f"Coeficiente R²     : {error_r2:.4f}")
print("=" * 50)

```

**c) si los modelos de gradiente descendente y ecuación normal son diferentes, ¿cuál es el motivo?, y si son iguales, ¿cual tiene mejor desempeño?**

Ambos modelos dan resultados casi identicos con una diferencia en su error de menos de 10\^-8 , pero dada la complejidad computacional de cada uno , lo mejor es tomar el de ecuaaciones normales ,que converge de forma mas rapida y con menos trabajo

**c) ¿Qué ventajas tiene el gradiente descendente sobre la ecuación normal?**

-Cuando se trata de grandes numeros de datos con muchas caracteristicas , el gradiente descendiente tiene una complejidad mucho menor a las ecuaciones normales , por lo que requiere de menoos trabajo computacional

-Si se trata de un grupo de datos muy grande con un gran numero de caracteristicas ,el gradiente descendiente lograra converger a un resultado de forma mas rapida que las ecuaciones normales

-Requiere de un uso menor de memoria al solo almacenar los pesos y los parametros, a diferencia de las ecuaciones normales que necesitan manipular y almacenar todas las matrices por completo

**d) Si el MSE no disminuye después de 200 épocas, ¿qué hiperparámetros ajustarías?**

Si el MSE no disminuye despues de 200 epocas es porque debe estar atascado debido a que la tasa de aprendizaje es muy baja , lo que genera que los cambios en los pesos sean muy pequeños o casi insignificantes , resultando en periodos muy largos para converger , o incluso se puede dar el caso de que no logre converger

### 4. Cross Validation

**b) ¿Qué ventajas tiene la validación cruzada sobre la división de los datos en entrenamiento y test?**

-Aprovechamiento total de los datos: Todos los datos se utilizan tanto para entrenar como para validar, lo que permite obtener una evaluación más representativa del modelo.

-Reducción del sesgo en la evaluación: Minimiza el impacto de una mala división de los datos, ya que el modelo se prueba con diferentes subconjuntos del dataset.

-Mayor utilidad en datasets pequeños: Es especialmente beneficiosa cuando hay pocos datos, ya que evita desperdiciar información al no reservar un único conjunto de test fijo.

Este enfoque garantiza una evaluación más robusta del desempeño del modelo y reduce la variabilidad en los resultados.

### 5. Evaluación del Modelo

**a) Evalúe el desempeño de los modelos implementados en los puntos 2 y 3, usando el error cuadrático medio (MSE) y el coeficiente de determinación (𝑅2). ¿Qué modelo tienemejor desempeño?**

Al comparar los resultados obtenidos con las ecuaciones normales y el descenso de gradiente, se observa que las ecuaciones normales ofrecen un modelo con un error cuadrático medio (MSE) menor, lo que indica un mejor ajuste en términos de minimización del error absoluto.

Además, el MSE mide el error en las mismas unidades que la variable objetivo, permitiendo interpretar directamente la magnitud de las desviaciones del modelo. En contraste, 𝑅\^2 es una métrica relativa, lo que significa que solo muestra qué porcentaje de la variabilidad de los datos es explicado por el modelo, sin proporcionar información sobre el error absoluto real.

En este caso, el MSE ha resultado ser una mejor métrica para evaluar el desempeño del modelo, ya que refleja con mayor precisión la calidad del ajuste en términos de los errores absolutos cometidos por la regresión.
